{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Network Example\n",
    "\n",
    "Builds a segmentation neural network with TensorFlow 2.0 eager execution ([code]())\n",
    "\n",
    "### Code Overview:\n",
    "1. Import libraries\n",
    "2. Import data for training and testing \n",
    "3. Define Model\n",
    "4. Define Loss function\n",
    "5. Define Training procedure\n",
    "6. Run Model code \n",
    "    a. Train model\n",
    "    b. Test model\n",
    "7. Plot accuracy of results\n",
    "\n",
    "## Example Code\n",
    "\n",
    "import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define DataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import cairo\n",
    "\n",
    "WIDTH = 128\n",
    "HEIGHT = 128\n",
    "CHANNELS = 3\n",
    "\n",
    "class DataGenerator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.size = 1000\n",
    "        self.ratio = 0.8\n",
    "        \n",
    "        self.WIDTH = WIDTH\n",
    "        self.HEIGHT = HEIGHT\n",
    "        self.CHANNELS = CHANNELS\n",
    "        \n",
    "        self.generate();\n",
    "\n",
    "    def generate_image(self):\n",
    "        ''' Randomly generates an image with random boxes '''    \n",
    "\n",
    "        data = np.zeros( (HEIGHT,WIDTH, 4), dtype=np.uint8 ) \n",
    "        surface = cairo.ImageSurface.create_for_data( data, cairo.FORMAT_ARGB32, WIDTH, HEIGHT )\n",
    "        ctx = cairo.Context( surface )\n",
    "\n",
    "        ctx.scale (WIDTH, HEIGHT) # Normalizing the canvas\n",
    "        ctx.set_source_rgb(0, 0, 0)\n",
    "        ctx.rectangle (0, 0, 1, 1)  # Rectangle(x0, y0, x1, y1) \n",
    "        ctx.fill()\n",
    "\n",
    "        # Create random colored boxes\n",
    "        for _ in range(50):\n",
    "            rc = np.random.rand(3)\n",
    "            ctx.set_source_rgb(rc[0], rc[1], rc[2])\n",
    "\n",
    "            r = np.random.rand(2)\n",
    "            ctx.translate (r[0], r[1])      # Changing the current transformation matrix\n",
    "            ctx.rectangle (0, 0, 0.1, 0.1)  # Rectangle(x0, y0, x1, y1)\n",
    "            ctx.fill()\n",
    "            ctx.translate (-r[0], -r[1])    # Changing the current transformation matrix\n",
    "\n",
    "        # Create a randomly placed red box\n",
    "        ctx.set_source_rgb(0, 0, 1)\n",
    "        r = np.random.rand(2)\n",
    "        ctx.translate (r[0], r[1])      # Changing the current transformation matrix\n",
    "        ctx.rectangle (0, 0, 0.1, 0.1)  # Rectangle(x0, y0, x1, y1)\n",
    "        ctx.fill()\n",
    "        ctx.translate (-r[0], -r[1])\n",
    "\n",
    "        img = data[:,:,0:3]\n",
    "        return img;\n",
    "\n",
    "    def whiten_data(self, features):\n",
    "        \"\"\" whiten dataset - zero mean and unit standard deviation \"\"\"\n",
    "        features = np.reshape(features, (self.size, WIDTH * HEIGHT * CHANNELS))\n",
    "        features = (np.swapaxes(features,0,1) - np.mean(features, axis=1)) / np.std(features, axis=1)\n",
    "        features = np.swapaxes(features,0,1)\n",
    "        features = np.reshape(features, (self.size, WIDTH, HEIGHT, CHANNELS))\n",
    "        #features = (features - np.mean(features, axis=0)) / np.std(features, axis=0)\n",
    "        return features\n",
    "\n",
    "    def unwhiten_img(self, img): \n",
    "        \"\"\" remove whitening for a single image \"\"\" \n",
    "        img = np.reshape(img, (WIDTH * HEIGHT * CHANNELS))\n",
    "        img = (img - np.min(img)) / (np.max(img) - np.min(img)) \n",
    "        img = np.reshape(img, (WIDTH, HEIGHT, CHANNELS))\n",
    "        return img\n",
    "\n",
    "    def generate(self):\n",
    "        ''' Generates a randomly generated dataset '''\n",
    "        img = self.generate_image()\n",
    "        self.data = np.stack( (img, self.generate_image()))\n",
    "        for _ in range(self.size - 2):\n",
    "            img = self.generate_image()\n",
    "            self.data = np.concatenate( (self.data, img[None,:]), axis=0)\n",
    "        \n",
    "        # Generate truth data\n",
    "        threshold = [200, 0, 0]\n",
    "        self.label = np.all(np.greater_equal(self.data, threshold), axis=3) * 1.0;\n",
    "        self.label = np.reshape(self.label, (self.size, WIDTH, HEIGHT, 1))\n",
    "        self.label = np.concatenate( (1 - self.label, self.label), axis=3) # Index 0: Incorrect, Index 1: Correct\n",
    "\n",
    "        # Setup data \n",
    "        self.data = self.whiten_data(self.data)\n",
    "\n",
    "        # Split data into test/training sets\n",
    "        index = int(self.ratio * len(self.data)) # Split index\n",
    "        self.x_train = self.data[0:index, :]\n",
    "        self.y_train = self.label[0:index]\n",
    "        self.x_test = self.data[index:,:]\n",
    "        self.y_test = self.label[index:]\n",
    "\n",
    "    def show(self, index):\n",
    "        ''' Show a data slice at index'''\n",
    "        img = self.unwhiten_img( self.data[index] )\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "\n",
    "    def show_label(self, index):\n",
    "        ''' Show a truth data slice at index'''\n",
    "        img = self.label[index]\n",
    "        plt.imshow(img[:,:,0], cmap='gray')\n",
    "        plt.show()  \n",
    "        plt.imshow(img[:,:,1], cmap='gray')\n",
    "        plt.show()  \n",
    "\n",
    "    def print(self):\n",
    "        print(\"Data Split: \", self.ratio)\n",
    "        print(\"Train => x:\", self.x_train.shape, \" y:\", self.y_train.shape)\n",
    "        print(\"Test  => x:\", self.x_test.shape, \" y:\", self.y_test.shape)\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        ''' Retrieves the next batch for a given batch size '''\n",
    "        length = self.x_train.shape[0]\n",
    "        indices = np.random.randint(0, length, batch_size) # Grab batch_size values randomly\n",
    "        return [self.x_train[indices], self.y_train[indices]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Split:  0.8\n",
      "Train => x: (800, 128, 128, 3)  y: (800, 128, 128, 2)\n",
      "Test  => x: (200, 128, 128, 3)  y: (200, 128, 128, 2)\n"
     ]
    }
   ],
   "source": [
    "# Generate Dataset\n",
    "data = DataGenerator()\n",
    "data.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Training and Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "num_epochs = 10\n",
    "display_step = 1\n",
    "batch_size = 32\n",
    "\n",
    "# Network Parameters\n",
    "WIDTH = data.WIDTH; HEIGHT = data.HEIGHT; CHANNELS = data.CHANNELS\n",
    "NUM_INPUTS = WIDTH * HEIGHT * CHANNELS\n",
    "NUM_OUTPUTS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Architecture\n",
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.c1 = Conv2D(64, (3, 3), padding=\"same\", activation=tf.nn.relu)\n",
    "        self.c2 = Conv2D(64, (3, 3), padding=\"same\", activation=tf.nn.relu)\n",
    "        self.mp = MaxPooling2D(padding=\"same\", strides=(2, 2), pool_size=(2, 2))\n",
    "        self.flatten = Flatten()\n",
    "        self.fc1 = Dense(128, activation=tf.nn.relu)\n",
    "        self.dropout = Dropout(0.2)\n",
    "        self.fc2 = Dense(128, activation=tf.nn.relu)\n",
    "        self.fcout = Dense(NUM_OUTPUTS, activation=tf.nn.softmax)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.c1(x)\n",
    "        x = self.c2(x)\n",
    "        x = self.mp(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.fcout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train(model, inputs, outputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss_value = loss(predictions, outputs)\n",
    "    grads = tape.gradient(loss_value, model.variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.variables))\n",
    "    train_loss(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test(model, inputs, outputs):\n",
    "    predictions = model(inputs)\n",
    "    test_accuracy(outputs, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Losses and Optimizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
